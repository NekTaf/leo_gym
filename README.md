
<h1>
  <img src="images/atellit%20(3).gif" alt="logo" width="100" style="vertical-align: middle; margin-left: 20px;">
  Low Earth Orbit (LEO)-GYM
</h1>


## Motivation 
LEO-GYM is a library for creating Reinforcement Learning (RL) environments for training policies to control satellites operating in LEO.



## Contents

1. **Library Introduction Tutorial**

2. **Across-Track (ACT) Maneuvers in LEO**
   1. Training of a policy to conduct ACT maneuvers in LEO  
   2. Reformulation of ACT maneuvers as a Semi-MDP to account for variable duration

3. **Collision Avoidance Maneuvers**


## Installation

It is recommended to first create a virtual environment. Then clone and install the library (use -e for editable mode):

```bash 
git clone https://github.com/NekTaf/leo_gym.git
 cd leo_gym
 pip install -e .
```

Alternatively, install directly from GitHub:

```bash
pip install git+https://github.com/NekTaf/leo_gym.git
```
## Acknowledgements
This work has been partially funded by the European Space Agency (ESA) open Invitations to Tender (ITT) and innovation research grant in OPTACOM project, in collaboration with OHB Sweden under Grant Contract no: OPC-OSE-CC-0536


## Contact

Nektarios Aristeidis Tafanidis: nektariostaf@gmail.com




<!-- # optacom


# Naming:

+ sk: station keeping
+ roe: relative orbital elements 
+ doe: damico orbital elements as defined in thesis
+ eci: earth centered inertial frame 
+ manplan: manuever plan with direction and duration, can include coasting phase


# Main objects:

+ Satellite dynamics
    + defines perturbations
    + orbit characteristics

+ Propagator:
    + used only be dynamics to propagate system

+ Satellite: 
    + used to interact with controllers and simulators
    + organize data collection from simulation
    + cam only initialize dynamics and call step propagation

+ Gym environments:
    + As defined in Gymnasium Farama
    + Can only interact with satellite objects and access the current or past data generated by simulation

+ NMPC:
    + Nonlinear model predictive controller 

# Reference state generation

+ Initialize satellite object and remove perturbations
+ Feed in the total trajectory to a perturbated satellite model as nominal data


# Experiment loggin in Reinforcement learning

+ Using mlflow
+ Loggs inlcude satellite module file, gym file and algorithms and environment configuration files 
+ Policies are saved every n steps not only the lats one

 -->
